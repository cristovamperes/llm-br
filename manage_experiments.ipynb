{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gerenciar Experimentos — v1, v2, v3\n\n",
    "Painel prático para:\n",
    "- Excluir modelos/mapeamentos gerados (todas as versões)\n",
    "- Retreinar v1, v2 (executando os notebooks) e v3 (script)\n",
    "- Gerar textos comparativos nas 3 versões e salvar relatório em `comparisons/`\n\n",
    "Observação: v1 e v2 rodam via execução dos notebooks; v3 roda via script.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SETUP\n",
    "import os, sys, shutil, subprocess, json, time\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "REPO = Path.cwd()\n",
    "V1 = REPO / 'versions' / 'v1-char-rnn'\n",
    "V2 = REPO / 'versions' / 'v2-char-lm'\n",
    "V3 = REPO / 'versions' / 'v3-brwac'\n",
    "\n",
    "NB_V1 = V1 / 'notebooks' / 'llm.ipynb'\n",
    "NB_V2 = V2 / 'notebooks' / 'llm_v2.ipynb'\n",
    "SCRIPT_V3 = V3 / 'scripts' / 'treinar_com_brwac.py'\n",
    "\n",
    "MODELS_DIRS = [V1/'models', V2/'models', V3/'models']\n",
    "MAPS_DIRS = [V1/'mappings', V2/'mappings', V3/'mappings']\n",
    "COMP_DIR = REPO / 'comparisons'\n",
    "COMP_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "print('Repo:', REPO)\n",
    "print('v1:', V1)\n",
    "print('v2:', V2)\n",
    "print('v3:', V3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UTIL: limpar artefatos (.keras/.pkl)\n",
    "def clean_artifacts():\n",
    "    removed = []\n",
    "    for d in MODELS_DIRS + MAPS_DIRS:\n",
    "        d.mkdir(parents=True, exist_ok=True)\n",
    "        for p in d.glob('*'):\n",
    "            if p.is_file():\n",
    "                try:\n",
    "                    p.unlink()\n",
    "                    removed.append(str(p))\n",
    "                except OSError:\n",
    "                    pass\n",
    "    return removed\n",
    "\n",
    "def _run(cmd: list, cwd: Path | None = None):\n",
    "    print('[RUN]', ' '.join(map(str, cmd)))\n",
    "    res = subprocess.run(cmd, cwd=cwd or REPO, text=True, capture_output=True)\n",
    "    print(res.stdout)\n",
    "    if res.returncode != 0:\n",
    "        print(res.stderr)\n",
    "        raise RuntimeError(f'Command failed: {cmd}')\n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RETREINO\n",
    "def retrain_v1():\n",
    "    # Executa o notebook v1 inplace\n",
    "    cmd = [sys.executable, '-m', 'jupyter', 'nbconvert', '--to', 'notebook', '--inplace', '--execute', str(NB_V1)]\n",
    "    return _run(cmd)\n",
    "\n",
    "def retrain_v2():\n",
    "    cmd = [sys.executable, '-m', 'jupyter', 'nbconvert', '--to', 'notebook', '--inplace', '--execute', str(NB_V2)]\n",
    "    return _run(cmd)\n",
    "\n",
    "def retrain_v3(epocas=10, max_textos=10000, tamanho_sequencia=160, tamanho_lstm=256, batch_size=256, validacao_split=0.05):\n",
    "    cmd = [sys.executable, str(SCRIPT_V3),\n",
    "           '--epocas', str(epocas),\n",
    "           '--max_textos', str(max_textos),\n",
    "           '--tamanho_sequencia', str(tamanho_sequencia),\n",
    "           '--tamanho_lstm', str(tamanho_lstm),\n",
    "           '--batch_size', str(batch_size),\n",
    "           '--validacao_split', str(validacao_split)]\n",
    "    return _run(cmd)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMPARACAO\n",
    "from compare_generate import run_compare, ModelSpec\n",
    "\n",
    "def _default_specs():\n",
    "    return [\n",
    "        ModelSpec('v1-char-rnn', 'versions/v1-char-rnn/models/modelo_char_rnn.keras', 'versions/v1-char-rnn/mappings/mapeamentos.pkl'),\n",
    "        ModelSpec('v2-char-lm', 'versions/v2-char-lm/models/modelo_char_lm_v2.keras', 'versions/v2-char-lm/mappings/mapeamentos_v2.pkl'),\n",
    "        ModelSpec('v3-brwac', 'versions/v3-brwac/models/modelo_brwac.keras', 'versions/v3-brwac/mappings/mapeamentos_brwac.pkl'),\n",
    "    ]\n",
    "\n",
    "def _filter_specs(specs, only):\n",
    "    if not only: return specs\n",
    "    keys = [k.strip().lower() for k in only.split(',') if k.strip()]\n",
    "    keep = []\n",
    "    for s in specs:\n",
    "        tag = 'v1' if 'v1-char-rnn' in s.name else ('v2' if 'v2-char-lm' in s.name else ('v3' if 'v3-brwac' in s.name else s.name))\n",
    "        if tag.lower() in keys: keep.append(s)\n",
    "    return keep\n",
    "\n",
    "def compare_and_save(prompts, length=400, temperature=0.8, only=''):\n",
    "    ts = datetime.now().strftime('%Y%m%d-%H%M%S')\n",
    "    out = []\n",
    "    specs = _filter_specs(_default_specs(), only)\n",
    "    for p in prompts:\n",
    "        out.append(f'PROMPT: {p}')\n",
    "        import io, contextlib\n",
    "        buf = io.StringIO()\n",
    "        with contextlib.redirect_stdout(buf):\n",
    "            run_compare(p, length, temperature, specs, seed=42)\n",
    "        out.append(buf.getvalue())\n",
    "        out.append('-'*80)\n",
    "    path = COMP_DIR / f'comparacao-{ts}.txt'\n",
    "    path.write_text('\n'.join(out), encoding='utf-8')\n",
    "    print('Salvo em:', path)\n",
    "    return path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uso Rápido\n\n",
    "- Para apagar artefatos: execute a célula abaixo.\n",
    "- Para retreinar: execute as funções `retrain_v1()`, `retrain_v2()`, `retrain_v3(...)`.\n",
    "- Para comparar: edite a lista de prompts e rode `compare_and_save(...)`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXCLUIR ARTEFATOS (use com cuidado)\n",
    "removed = clean_artifacts()\n",
    "print('Removidos:', len(removed))\n",
    "for p in removed[:10]:\n",
    "    print(' -', p)\n",
    "if len(removed) > 10:\n",
    "    print(' ...')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXEMPLOS DE RETREINO\n",
    "# retrain_v1()\n",
    "# retrain_v2()\n",
    "# retrain_v3(epocas=10, max_textos=10000, tamanho_sequencia=160, tamanho_lstm=512, batch_size=256, validacao_split=0.05)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMPARACOES\n",
    "prompts = [\n",
    "    'O trabalho de conclusão de curso deve',\n",
    "    'O dia-a-dia de um homem',\n",
    "    'O trabalho da mulher',\n",
    "    'Sempre vou buscar',\n",
    "]\n",
    "# path = compare_and_save(prompts, length=400, temperature=0.8)\n",
    "# print('Relatório:', path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
