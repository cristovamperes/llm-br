{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 1 - Setup\n\nDefine caminhos padronizados para artefatos desta versão (v2).\n\n- MODELO_OUT → versions/v2-char-lm/models/modelo_char_lm_v2.keras\n- MAPEAMENTOS_OUT → versions/v2-char-lm/mappings/mapeamentos_v2.pkl\n\nEsses caminhos organizam os arquivos de treino e facilitam reuso."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# SETUP_ARTIFACT_PATHS\n",
    "from pathlib import Path\n",
    "BASE_DIR = Path.cwd().resolve()\n",
    "if BASE_DIR.name == 'notebooks':\n",
    "    BASE_DIR = BASE_DIR.parent\n",
    "MODELS_DIR = BASE_DIR / 'models'\n",
    "MAPPINGS_DIR = BASE_DIR / 'mappings'\n",
    "MODELS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "MAPPINGS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "MODELO_OUT = MODELS_DIR / 'modelo_char_lm_v2.keras'\n",
    "MAPEAMENTOS_OUT = MAPPINGS_DIR / 'mapeamentos_v2.pkl'\n",
    "print('Modelo:', MODELO_OUT)\n",
    "print('Mapeamentos:', MAPEAMENTOS_OUT)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 2 - Dados e Pre-Processamento\n\nCarrega o corpus (ex.: Dom Casmurro) e aplica normalizações: lower() e compressão de espaços. Isso reduz ruído e ajuda modelos por caracteres a convergirem melhor."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import tensorflow as tf, os, pickle\n",
    "URL_LIVRO = 'https://www.gutenberg.org/files/55752/55752-0.txt'\n",
    "caminho_arquivo = tf.keras.utils.get_file('dom_casmurro.txt', URL_LIVRO)\n",
    "with open(caminho_arquivo, 'rb') as f:\n",
    "    texto = f.read().decode('utf-8', errors='ignore')\n",
    "texto = texto.lower()\n",
    "texto = ' '.join(texto.split())\n",
    "print('Tamanho do corpus (chars):', len(texto))\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 3 - Vocabulario e Janelas de Treino\n\nCria vocabulário e mapeamentos char→id/id→char (salvos em MAPEAMENTOS_OUT). Define SEQ_LEN=160 e um gerador one-hot de janelas e rótulos sob demanda."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "SEQ_LEN = 160\n",
    "caracteres = sorted(list(set(texto)))\n",
    "char_to_id = {c: i for i, c in enumerate(caracteres)}\n",
    "id_to_char = {i: c for i, c in enumerate(caracteres)}\n",
    "vocab = len(caracteres)\n",
    "with open(MAPEAMENTOS_OUT, 'wb') as f:\n",
    "    pickle.dump({'char_to_id': char_to_id, 'id_to_char': id_to_char, 'tamanho_sequencia': SEQ_LEN}, f)\n",
    "print('Vocabulario:', vocab, 'caracteres')\n",
    "\n",
    "def batch_generator(texto, char2idx, seq_len=160, batch_size=256, step=1):\n",
    "    n = len(texto) - seq_len\n",
    "    V = len(char2idx)\n",
    "    i = 0\n",
    "    while True:\n",
    "        X = np.zeros((batch_size, seq_len, V), dtype=np.float32)\n",
    "        y = np.zeros((batch_size,), dtype=np.int32)\n",
    "        for b in range(batch_size):\n",
    "            idx = (i + b*step) % n\n",
    "            seq = texto[idx: idx+seq_len\n",
    "            nxt = texto[idx+seq_len]\n",
    "            for t, ch in enumerate(seq):\n",
    "                X[b, t, char2idx.get(ch, 0)] = 1.0\n",
    "            y[b] = char2idx.get(nxt, 0)\n",
    "        i = (i + batch_size*step) % n\n",
    "        yield X, y\n",
    "\n",
    "steps_per_epoch = max(1, (len(texto) - SEQ_LEN) // 256)\n",
    "print('steps_per_epoch (aprox):', steps_per_epoch)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 4 - Arquitetura do Modelo\n\nLSTM(512) seguida de Dense(vocab, softmax). Entrada one-hot (seq_len × vocab)."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from tensorflow.keras import models, layers, optimizers\n",
    "model = models.Sequential([\n",
    "    layers.LSTM(512, input_shape=(SEQ_LEN, vocab)),\n",
    "    layers.Dense(vocab, activation='softmax'),\n",
    "])\n",
    "optimizer = optimizers.Adam(learning_rate=2e-3, clipnorm=1.0)\n",
    "model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 5 - Treinamento\n\nCallbacks: ModelCheckpoint, ReduceLROnPlateau e EarlyStopping. Parâmetros sugeridos: BATCH_SIZE=256, EPOCHS=40."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from tensorflow.keras import callbacks as kb\n",
    "BATCH_SIZE = 256\n",
    "EPOCHS = 40\n",
    "monitor='loss'\n",
    "cb = [\n",
    "    kb.ModelCheckpoint(str(MODELO_OUT), save_best_only=True, monitor=monitor, mode='min'),\n",
    "    kb.ReduceLROnPlateau(monitor=monitor, factor=0.5, patience=3, min_lr=5e-5),\n",
    "    kb.EarlyStopping(monitor=monitor, patience=5, restore_best_weights=True),\n",
    "]\n",
    "gen = batch_generator(texto, char_to_id, seq_len=SEQ_LEN, batch_size=BATCH_SIZE, step=1)\n",
    "steps_per_epoch = max(1, (len(texto) - SEQ_LEN) // BATCH_SIZE)\n",
    "history = model.fit(gen, steps_per_epoch=steps_per_epoch, epochs=EPOCHS, callbacks=cb, verbose=1)\n",
    "print('Treinamento concluido! Melhor modelo salvo em:', MODELO_OUT)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 6 - Salvamento e Carregamento\n\nRecarrega o modelo/mapeamentos para gerar texto sem retreinar."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import tensorflow as tf, pickle\n",
    "modelo = tf.keras.models.load_model(MODELO_OUT) if MODELO_OUT.exists() else model\n",
    "with open(MAPEAMENTOS_OUT, 'rb') as f:\n",
    "    maps = pickle.load(f)\n",
    "c2i = maps['char_to_id']\n",
    "i2c = maps['id_to_char']\n",
    "print('Artefatos prontos para gerar texto.')\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 7 - Geracao de Texto\n\nGeração com top-k (k=20) e temperatura 0.8. Use seed (~160 chars) do corpus."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "def sample_top_k(probs, k=20, temperature=0.8, rng=None):\n",
    "    probs = np.asarray(probs, dtype=np.float64)\n",
    "    if rng is None: rng = np.random.default_rng()\n",
    "    k = int(max(1, min(k, probs.size)))\n",
    "    top_idx = np.argpartition(-probs, k-1)[:k]\n",
    "    sel = probs[top_idx]\n",
    "    if temperature > 0:\n",
    "        logits = np.log(np.maximum(sel, 1e-9)) / temperature\n",
    "        logits -= logits.max()\n",
    "        sel = np.exp(logits)\n",
    "    p = sel / sel.sum()\n",
    "    return int(top_idx[rng.choice(len(top_idx), p=p)])\n",
    "\n",
    "def gerar_texto(model, char_to_id, id_to_char, seed, comprimento=400, k=20, temperatura=0.8):\n",
    "    vocab = len(id_to_char)\n",
    "    seq_len = model.input_shape[1] if isinstance(model.input_shape, (list,tuple)) else 160\n",
    "    rep = ' ' if ' ' in char_to_id else next(iter(char_to_id.keys()))\n",
    "    seed = ''.join(ch if ch in char_to_id else rep for ch in seed)\n",
    "    ids = [char_to_id[ch] for ch in seed][-seq_len:]\n",
    "    if len(ids) < seq_len:\n",
    "        pad = char_to_id.get(' ', ids[0] if ids else 0)\n",
    "        ids = [pad] * (seq_len - len(ids)) + ids\n",
    "    out = []\n",
    "    rng = np.random.default_rng(42)\n",
    "    for _ in range(comprimento):\n",
    "        X = np.zeros((1, seq_len, vocab), dtype=np.float32)\n",
    "        for t, idx in enumerate(ids):\n",
    "            if idx < vocab: X[0, t, idx] = 1.0\n",
    "        p = model.predict(X, verbose=0)[0]\n",
    "        nxt = sample_top_k(p, k=k, temperature=temperatura, rng=rng)\n",
    "        out.append(id_to_char.get(nxt, '?'))\n",
    "        ids = ids[1:] + [nxt]\n",
    "    return ''.join(out)\n",
    "\n",
    "# Exemplo: seed = '...'\n",
    "# print(gerar_texto(modelo, c2i, i2c, seed, comprimento=400, k=20, temperatura=0.8))\n"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}