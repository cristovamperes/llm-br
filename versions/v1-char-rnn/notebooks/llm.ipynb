{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 1 - Setup\n\nNesta seção configuramos caminhos padronizados para salvar o modelo (.keras) e os mapeamentos (.pkl) dentro desta versão.\nEsses caminhos garantem reprodutibilidade e ajudam a manter o repositório organizado."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# SETUP_ARTIFACT_PATHS\n",
    "from pathlib import Path\n",
    "BASE_DIR = Path.cwd().resolve()\n",
    "if BASE_DIR.name == 'notebooks':\n",
    "    BASE_DIR = BASE_DIR.parent\n",
    "MODELS_DIR = BASE_DIR / 'models'\n",
    "MAPPINGS_DIR = BASE_DIR / 'mappings'\n",
    "MODELS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "MAPPINGS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "MODELO_OUT = MODELS_DIR / 'modelo_char_rnn.keras'\n",
    "MAPEAMENTOS_OUT = MAPPINGS_DIR / 'mapeamentos.pkl'\n",
    "print('Modelo:', MODELO_OUT)\n",
    "print('Mapeamentos:', MAPEAMENTOS_OUT)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 2 - Dados e Pre-Processamento\n\nCarregamos o corpus (Dom Casmurro, Project Gutenberg) e aplicamos normalizações simples:\n- converter para minúsculas; \n- comprimir espaços em branco.\nEssas normalizações reduzem o vocabulário e o ruído, melhorando o aprendizado em modelos por caracteres."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import tensorflow as tf, os, pickle\n",
    "URL_LIVRO = 'https://www.gutenberg.org/files/55752/55752-0.txt'\n",
    "caminho_arquivo = tf.keras.utils.get_file('dom_casmurro.txt', URL_LIVRO)\n",
    "with open(caminho_arquivo, 'rb') as f:\n",
    "    texto = f.read().decode('utf-8', errors='ignore')\n",
    "# Normalizacao\n",
    "texto = texto.lower()\n",
    "texto = ' '.join(texto.split())\n",
    "print('Tamanho do corpus (chars):', len(texto))\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 3 - Vocabulario e Janelas de Treino\n\nCriamos o vocabulário de caracteres e os mapeamentos `char→id` e `id→char` (salvos em `MAPEAMENTOS_OUT`).\n\nTambém definimos o comprimento da janela de entrada (tamanho de sequência) e preparamos um gerador de lotes que cria as janelas de treino e os rótulos (próximo caractere) sob demanda, economizando memória."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "TAMANHO_SEQUENCIA = 160\n",
    "caracteres = sorted(list(set(texto)))\n",
    "char_para_int = {c: i for i, c in enumerate(caracteres)}\n",
    "int_para_char = {i: c for i, c in enumerate(caracteres)}\n",
    "n_vocabulario = len(caracteres)\n",
    "with open(MAPEAMENTOS_OUT, 'wb') as f:\n",
    "    pickle.dump({'char_para_int': char_para_int, 'int_para_char': int_para_char, 'tamanho_sequencia': TAMANHO_SEQUENCIA}, f)\n",
    "print('Vocabulario:', n_vocabulario, 'caracteres')\n",
    "\n",
    "def batch_generator(texto, char2idx, seq_len=160, batch_size=256, step=1):\n",
    "    n = len(texto) - seq_len\n",
    "    vocab = len(char2idx)\n",
    "    i = 0\n",
    "    while True:\n",
    "        X = np.zeros((batch_size, seq_len, vocab), dtype=np.float32)\n",
    "        y = np.zeros((batch_size,), dtype=np.int32)\n",
    "        for b in range(batch_size):\n",
    "            idx = (i + b*step) % n\n",
    "            seq = texto[idx: idx+seq_len]\n",
    "            nxt = texto[idx+seq_len]\n",
    "            for t, ch in enumerate(seq):\n",
    "                X[b, t, char2idx.get(ch, 0)] = 1.0\n",
    "            y[b] = char2idx.get(nxt, 0)\n",
    "        i = (i + batch_size*step) % n\n",
    "        yield X, y\n",
    "\n",
    "steps_per_epoch = max(1, (len(texto) - TAMANHO_SEQUENCIA) // 256)\n",
    "print('steps_per_epoch (aprox):', steps_per_epoch)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 4 - Arquitetura do Modelo\n\nUsamos uma LSTM seguida de `Dense(vocab, softmax)`. A entrada é one-hot (seq_len × vocab).\n\nEsta arquitetura simples é adequada para fins didáticos de LM por caracteres."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from tensorflow.keras import models, layers, optimizers\n",
    "model = models.Sequential([\n",
    "    layers.LSTM(256, input_shape=(TAMANHO_SEQUENCIA, n_vocabulario)),\n",
    "    layers.Dense(n_vocabulario, activation='softmax'),\n",
    "])\n",
    "optimizer = optimizers.Adam(learning_rate=2e-3, clipnorm=1.0)\n",
    "model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 5 - Treinamento\n\nTreinamos com callbacks para manter o melhor modelo e ajustar a taxa de aprendizado:\n- ModelCheckpoint (salva o melhor); \n- ReduceLROnPlateau (reduz LR em platôs);\n- EarlyStopping (para cedo e restaura os melhores pesos)."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from tensorflow.keras import callbacks as kb\n",
    "BATCH_SIZE = 256\n",
    "EPOCAS_TREINO = 40\n",
    "monitor='loss'\n",
    "cb = [\n",
    "    kb.ModelCheckpoint(str(MODELO_OUT), save_best_only=True, monitor=monitor, mode='min'),\n",
    "    kb.ReduceLROnPlateau(monitor=monitor, factor=0.5, patience=3, min_lr=5e-5),\n",
    "    kb.EarlyStopping(monitor=monitor, patience=5, restore_best_weights=True),\n",
    "]\n",
    "gen = batch_generator(texto, char_para_int, seq_len=TAMANHO_SEQUENCIA, batch_size=BATCH_SIZE, step=1)\n",
    "steps_per_epoch = max(1, (len(texto) - TAMANHO_SEQUENCIA) // BATCH_SIZE)\n",
    "history = model.fit(gen, steps_per_epoch=steps_per_epoch, epochs=EPOCAS_TREINO, callbacks=cb, verbose=1)\n",
    "print('Treinamento concluido! Melhor modelo salvo em:', MODELO_OUT)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 6 - Salvamento e Carregamento\n\nOs artefatos ficam em `models/` e `mappings/` desta versão. Abaixo um exemplo de recarregamento para geração sem retreinar."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import tensorflow as tf, pickle\n",
    "modelo = tf.keras.models.load_model(MODELO_OUT) if MODELO_OUT.exists() else model\n",
    "with open(MAPEAMENTOS_OUT, 'rb') as f:\n",
    "    maps = pickle.load(f)\n",
    "c2i = maps['char_para_int']\n",
    "i2c = maps['int_para_char']\n",
    "print('Artefatos prontos para gerar texto.')\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 7 - Geracao de Texto\n\nUsamos amostragem top-k (k=20) por padrão, com temperatura 0.8. Forneça uma seed (~160 caracteres) do próprio corpus para melhor fluência.\n\nConceitos:\n- Temperatura controla a aleatoriedade (0.7–0.9 sugerido);\n- Top-k restringe a escolha aos k tokens mais prováveis, evitando repetições."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "def sample_top_k(probs, k=20, temperature=0.8, rng=None):\n",
    "    probs = np.asarray(probs, dtype=np.float64)\n",
    "    if rng is None: rng = np.random.default_rng()\n",
    "    k = int(max(1, min(k, probs.size)))\n",
    "    top_idx = np.argpartition(-probs, k-1)[:k]\n",
    "    sel = probs[top_idx]\n",
    "    if temperature > 0:\n",
    "        logits = np.log(np.maximum(sel, 1e-9)) / temperature\n",
    "        logits -= logits.max()\n",
    "        sel = np.exp(logits)\n",
    "    p = sel / sel.sum()\n",
    "    return int(top_idx[rng.choice(len(top_idx), p=p)])\n",
    "\n",
    "def gerar_texto(model, char_to_id, id_to_char, seed, comprimento=400, k=20, temperatura=0.8):\n",
    "    vocab = len(id_to_char)\n",
    "    seq_len = model.input_shape[1] if isinstance(model.input_shape, (list,tuple)) else 160\n",
    "    rep = ' ' if ' ' in char_to_id else next(iter(char_to_id.keys()))\n",
    "    seed = ''.join(ch if ch in char_to_id else rep for ch in seed)\n",
    "    ids = [char_to_id[ch] for ch in seed][-seq_len:]\n",
    "    if len(ids) < seq_len:\n",
    "        pad = char_to_id.get(' ', ids[0] if ids else 0)\n",
    "        ids = [pad] * (seq_len - len(ids)) + ids\n",
    "    out = []\n",
    "    rng = np.random.default_rng(42)\n",
    "    for _ in range(comprimento):\n",
    "        X = np.zeros((1, seq_len, vocab), dtype=np.float32)\n",
    "        for t, idx in enumerate(ids):\n",
    "            if idx < vocab: X[0, t, idx] = 1.0\n",
    "        p = model.predict(X, verbose=0)[0]\n",
    "        nxt = sample_top_k(p, k=k, temperature=temperatura, rng=rng)\n",
    "        out.append(id_to_char.get(nxt, '?'))\n",
    "        ids = ids[1:] + [nxt]\n",
    "    return ''.join(out)\n",
    "\n",
    "# Exemplo de uso (edite 'seed' com ~160 chars do corpus):\n",
    "# seed = '...'\n",
    "# print(gerar_texto(modelo, c2i, i2c, seed, comprimento=400, k=20, temperatura=0.8))\n"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}